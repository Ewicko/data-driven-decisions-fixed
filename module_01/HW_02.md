---
jupytext:
  formats: md:myst,ipynb
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.4
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

```{code-cell} ipython3
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
plt.style.use('fivethirtyeight')

from numpy.random import default_rng
```

# HW_02

## Calculating and reporting $\pi$

Calculate your own value of $\pi$ using the random x- and y-coordinates. You need to report your findings to other engineers in your field. You should explain _why_ and _how_ you are calculating $\pi$, then describe the data with at least one plot. You can use more if it helps to explain the process or a point you are trying to make, here are some examples from our first $\pi$ calculation.

```{code-cell} ipython3
rng = default_rng(42)
plt.figure()

ax1 = plt.subplot(2, 2, (1, 2))
for N in range(100,1001, 100):
    trials = 100
    pi_trial = np.zeros(trials)
    for i in range(trials):
        x = rng.random(N)
        y = rng.random(N)

        r = x**2 + y**2

        pi_trial[i] = np.sum(r < 1**2)/N*4

    mean_pi = np.mean(pi_trial)
    std_pi = np.std(pi_trial)

    ax1.plot(N*np.ones(trials), pi_trial, 'ko', alpha = 0.1)
plt.title(r'Value of $\pi=${:1.3f}$\pm${:1.3f}'.format(mean_pi, std_pi));
ax2 = plt.subplot(2, 2, 3)
ax2.hist(pi_trial, 20, density=True)


x = np.linspace(3, 3.3)
pi_pdf = stats.norm.pdf(x, loc = mean_pi, scale = std_pi)
ax2.plot(x, pi_pdf)

ax3 = plt.subplot(2, 2, 4)
ax3.boxplot(pi_trial, vert=False);
```

```{code-cell} ipython3
default_rng?
```

```{code-cell} ipython3
ax1 = plt.subplot(1, 1, 1)
for N in range(100,1001, 100):
    trials = 100
    pi_trial = np.zeros(trials)
    for i in range(trials):
        x = rng.random(N)
        y = rng.random(N)

        r = x**2 + y**2

        pi_trial[i] = np.sum(r < 1**2)/N*4

    mean_pi = np.mean(pi_trial)
    std_pi = np.std(pi_trial)
    plt.plot(N*np.ones(trials), pi_trial, 'ko', alpha = 0.1)
```

## Some points to consider in your report

- $\pi$ is not a random variable, but the probability that you land in a circle vs a square if your coordinates are x = 0-r and y = 0-r is $\frac{A_{circle}}{A_{square}} = \frac{\pi r^2}{4r^2} = \frac{\pi}{4}$.
- When is the mean "close enough" to $\pi$?
- What do the tails of the calculated $\pi$ values tell us about the prediction?
- What else do you notice in your analysis?

+++

# Estimating the Value of pi Using Random Coordinates
## Introduction
In this report, we aim to estimate the value of pi using a Monte Carlo simulation. This method involves generating random coordinates (x, y) within a unit square and determining the ratio of points that fall inside a quarter of a circle inscribed within that square. This resulting ratio is directly proportional to pi (it is 1/4th of pi). Thus, we can use the random generation to find not only the probability that the random coordinate is in the circle, but an actual value of pi

## Methodology
Random Coordinate Generation: We generate random (x, y) coordinates within the interval [0, 1] using a random number generator implemented with numpy (randomly generated data)
.
###### Circle-Square Ratio: 
For each point, we calculate the distance from the origin (0, 0) using r= x^2+y^2. Points with r‚â§1 lie inside the circle and points with r>1 are classified as outside the circle. The ratio of the number of points inside the circle to the total number of points, multiplied by 4, gives an estimate of pi. 

**pi‚âàNumber¬†of¬†points¬†inside¬†circle/Total¬†number¬†of¬†points*4**

I simulated the value of pi for varying sample sizes (N = 100 to 1000 in increments of 100) and repeated each experiment 100 times to account for variability. The code can be seen above.


## Results and Analysis
### Data Visualization Interpretation
The figures presented above illustrate the outcomes of the Monte Carlo simulations used to estimate the value of pi with varying sample sizes (N ranging from 100 to 1000).

###### Scatter Plot Analysis:
- **Distribution of pi Estimates Across Sample Sizes**: The scatter plot shows a concentration of pi estimates as the sample size increases. At lower sample sizes (N=100, N=200), the variability in pi estimates is higher, evidenced by a broader spread of data points. As N increases (N=800, N=1000), the estimates cluster more tightly around the mean value, suggesting improved accuracy and reduced variability.

###### Histogram and Density Plot:
- **Normal Distribution Approximation**: The histogram of pi estimates at the maximum sample size (N=1000) suggests a normal distribution, confirmed by the overlay of the probability density function (red curve). This alignment with a normal distribution supports the Central Limit Theorem's prediction as the sample size increases. The Central Limit Theorem (CLT) applies primarily to the pi estimates, not directly to the individual values of r^2 = x^2 + y^2. Each trial in the simulation‚Äîdetermining if a point is inside or outside the quarter-circle‚Äîis a Bernoulli trial (random experiment with exactly two possible outcomes), and when you average the results of a large number of such independent trials, the distribution of these averages approximate a normal distribution, as predicted by the CLT. Although r^2 for individual points is derived from non-uniform distributions (since x^2 and y^2 are biased towards smaller values), the average of many r^2 values over numerous trials may exhibit normal-like characteristics due to the CLT, specifically when the number of trials is greater than ~30 https://www.investopedia.com/terms/c/central_limit_theorem.asp).

###### Boxplot:
- **Visual Summary of pi Estimates**: The boxplot provides a concise visual summary of the distribution of pi estimates at the maximum sample size. The median is indicated by the red line within the box, while the whiskers extend to show the range excluding outliers. This plot emphasizes the consistency and precision of the estimates as reflected by the compactness of the box and the minimal number of outliers.

#### Statistical Metrics
- **Accuracy**: Accuracy is assessed by comparing the calculated mean value of pi estimates to the actual value of pi (3.14159...). From the scatter plot's title, the mean pi estimate is 3.142, with an overall uncertainty of ¬±0.056. Thus, the accuracy, calculated as the difference between the mean estimate and the actual value of pi, is 3.142 - 3.14159 = 0.00041. This small deviation signifies a high degree of accuracy.

- **Precision**: Precision in this context is assessed by the standard deviation of the pi estimates, indicated by the uncertainty of ¬±0.056. Alternatively, examining the range of pi values in the boxplot, we observe that most data points lie within a narrow range, highlighting a high precision in the estimates. The compactness of the box in the boxplot and the small interquartile range further confirm the high precision of the method, particularly at higher sample sizes.


## Discussion

Although pi itself is a constant and not a random variable, the method used to estimate pi here relies on the probabilistic relationship between the areas of a circle and a square. The idea is based on the following ratio if a circle is inscribed in the square:

**ùê¥_circle = pi*r^2**

**ùê¥_square = (2*r)^2**

Given that we are dealing with a quarter circle of radius 1 inscribed in a unit square, the probability of a randomly chosen point (x, y) landing inside the quarter circle is the same as the ratio of their areas, which is pi/4. By counting the number of points that fall inside the quarter circle and scaling this proportion by 4, we obtain an estimate of pi.

### Mean Estimation: "Close Enough" to pi

The mean estimate of pi can be considered "close enough" is based on the application, but for a application that requires general engineering precision, a value of 3.14 is usually used (https://www.nextengineers.org/diy/pi-day-getting-know-pi#:~:text=%CF%80%20is%20a%20mathematical%20constant%20with%20an%20approximate%20value%20of%203.14.). Statistically, if the mean estimate falls within a confidence interval that includes the true value of pi, the estimate can be considered accurate. For instance, a 95% confidence interval that brackets 3.14159 indicates that our mean estimate is close enough. This is because the 95% confidence interval is the most widely used confidence interval in engineering and statistics, with 99% CI being used only occasionally (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5723800/#:~:text=The%2095%25%20confidence%20level%20is,than%20the%20narrower%2095%25%20CI.) Thus, we can back calculate the tolerance level such that the value of 3.14 falls within the 95% confidence interval of the Monte Carlo estimate of pi

Given:
- The mean estimate of œÄ is denoted as pi_bar.
- The standard deviation of these estimates is denoted as sigma.
- The number of trials (or simulations) is N.

The 95% confidence interval for the mean estimate can be calculated using the formula:
\CI = pi_bar +- z * sigma/sqrt(N)
where z is the z-score for a 95% confidence level (which is approximately 1.96 for a two-tailed test).

###### Calculation:
3.14 = pi_bar +- 1.96 * sigma/sqrt(N)
This can be rearranged to find sigma = (abs(pi_bar-3.14)* sqrtN)/1.96
With pi_bar = 3.142 and N = 1000 (for calculation purposes), sigma ~ 0.0253


A standard deviation sigma of approximately 0.0253 would ensure that the interval \[\3.142 - 0.0495, 3.142 + 0.0495\] includes 3.14, covering a range that extends beyond this to provide a 95% confidence level of containing the true œÄ estimate. 

### How to Tailor the Estimate to Different Tolerance Levels:

As the number of samples (N) increases (ei the number of points increases), the mean of the estimates approaches the true value of pi, and the standard deviation decreases. This means the estimates become more consistent and reliable. Thus, in order to change the tolerance, you can increase the number of samples until you are witin tolorence.


## Conclusion
The Monte Carlo method for estimating pi demonstrates the application of probabilistic and statistical principles in practical problem-solving. By understanding the relationship between the areas of a circle and a square and leveraging random sampling, we can obtain reliable estimates of pi. The discussion highlights the importance of sample size, variability, and the convergence properties of statistical estimates, providing valuable insights into the behavior and accuracy of simulation-based methods. This Monte Carlo approach provides a practical way to estimate pi and similar constants in various fields that need to be obtained emperically, including engineering, physics, and finance, where analytical solutions may be difficult or impossible to obtain.

```{code-cell} ipython3

```
